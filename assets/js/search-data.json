{
  
    
        "post0": {
            "title": "Getting Started with Kaggle Competitions: Melanoma Classification Challenge",
            "content": ". This blog is jointly co-authored by Pranjal Chitale. We both participated in the competition as a team. . . This post assumes that you are acquainted with the basic skills of working with PyTorch. If you are new to PyTorch, we would highly encourage you to go through Deep Leaning With PyTorch: A 60 Minute Blitz by PyTorch. It’s a great place for beginners to get your hands dirty. . Download data . Here we will be using the preprocessed images by Arnaud Roussel due to storage limitations on Google Colab. . Now let’s download the preprocessed image dataset using the Kaggle API. Remember to add your USERNAME and API_KEY in the code block below: . !pip install kaggle -q !mkdir /root/.kaggle !echo &#39;{&quot;username&quot;:&quot;YOUR_USERNAME&quot;,&quot;key&quot;:&quot;YOUR_API_KEY&quot;}&#39; &gt; /root/.kaggle/kaggle.json !chmod 600 /root/.kaggle/kaggle.json !kaggle datasets download -d arroqc/siic-isic-224x224-images !mkdir /content/siic-isic-224x224-images !unzip -q /content/siic-isic-224x224-images.zip -d /content/siic-isic-224x224-images . Download the csv files from the competition page and place this files in the content directory. . !python3 -m pip install --upgrade pip -q !pip install efficientnet_pytorch pretrainedmodels -q . What is Melanoma? . Malignant Melanoma is a type of skin cancer that develops from pigment-producing cells known as melanocytes. . The skin cells found in the upper layer of the skin are termed as Melanocytes. These produce a pigment Melanin, which is the pigment that is responsible for skin color. Exposure to UV radiation from the sun or tanning beds causes skin damage as it triggers these melanocytes to increase the secretion of Melanin. . Melanoma occurs when there is DNA damage caused by burning or tanning due to UV exposure, triggering mutations in the melanocytes leading to unrestricted cellular growth. . Objective . The objective of this competition is to identify melanoma in images of skin lesions. In particular, we need to use images within the same patient and determine which are likely to represent a melanoma. Using patient-level contextual information may help the development of image analysis tools, which could better support clinical dermatologists. . Melanoma is a deadly disease, but if detected at an early stage, most melanomas can be cured with minor surgery. . This competition is aimed at building a Classification Model that can predict whether the onset of malignant Melanoma from lesion images. . In short, we need to create a classification model that is capable of distinguishing whether the lesion in the image is benign (class 0) or malignant (class 1). . This will be very helpful to detect the early signs so that further medical attention can be made available to the patient. . Now let’s import the necessary packages below: . %matplotlib inline import os from tqdm import tqdm import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.model_selection import StratifiedKFold from sklearn.metrics import roc_auc_score import torch import torch.nn as nn import torch.nn.functional as F import torchvision.transforms as transforms import albumentations import pretrainedmodels from efficientnet_pytorch import EfficientNet from PIL import Image . Now, we select the device on which our network will run. Neural style transfer algorithm runs faster on GPU so check if GPU is available using torch.cuda.is_available(). . device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) . About the Dataset . The dataset consists of images and metadata, which are described as follows: . Images: DICOM, JPEG, TFRecord formats | Metadata: image_name, patient_id, sex, age_approx, anatom_site_general_challenge, diagnosis, benign_malignant, target | . Let’s take a look at the dataset: . train_images_path = &#39;./siic-isic-224x224-images/train/&#39; test_images_path = &#39;./siic-isic-224x224-images/test/&#39; train_df_path = &#39;./train.csv&#39; test_df_path = &#39;./test.csv&#39; . train_df = pd.read_csv(train_df_path) test_df = pd.read_csv(test_df_path) . train_df.head(5) . image_name patient_id sex age_approx anatom_site_general_challenge diagnosis benign_malignant target . 0 ISIC_2637011 | IP_7279968 | male | 45.0 | head/neck | unknown | benign | 0 | . 1 ISIC_0015719 | IP_3075186 | female | 45.0 | upper extremity | unknown | benign | 0 | . 2 ISIC_0052212 | IP_2842074 | female | 50.0 | lower extremity | nevus | benign | 0 | . 3 ISIC_0068279 | IP_6890425 | female | 45.0 | head/neck | unknown | benign | 0 | . 4 ISIC_0074268 | IP_8723313 | female | 55.0 | upper extremity | unknown | benign | 0 | . Let’s take a look at the number of samples in the train and test set: . print(f&quot;Train data shape: {train_df.shape}&quot;) print(f&quot;Test data shape: {test_df.shape}&quot;) . Train data shape: (33126, 8) Test data shape: (10982, 6) . Let’s take a look at the missing value count for each attribute: . train_df.isnull().sum() . image_name 0 patient_id 0 sex 65 age_approx 68 anatom_site_general_challenge 527 diagnosis 0 benign_malignant 0 target 0 dtype: int64 . We observe that the metadata contains several missing values. Imputation strategies like replacing with mean or k-nearest neighbors could be used. However, we did not go ahead with the same as we feel that it might induce some bias and negatively influence the classifier. . train_df[&#39;image_path&#39;] = train_df[&#39;image_name&#39;].apply(lambda img_name: os.path.join(train_images_path, img_name + &#39;.png&#39;)).values test_df[&#39;image_path&#39;] = test_df[&#39;image_name&#39;].apply(lambda img_name: os.path.join(test_images_path, img_name + &#39;.png&#39;)).values test_df.to_csv(&#39;test.csv&#39;, index=False) . Let’s take a look at the sample images of both classes: . def plot_images(data, target, nrows=3, ncols=3): data = data[data[&#39;target&#39;] == target].sample(nrows * ncols)[&#39;image_path&#39;] plt.figure(figsize=(nrows * 2, ncols * 2)) for idx, image_path in enumerate(data): image = Image.open(image_path) plt.subplot(nrows, ncols, idx + 1) plt.imshow(image) plt.axis(&#39;off&#39;) plt.show(); . plot_images(train_df, target=0) . plot_images(train_df, target=1) . Let’s take a look at the distribution of target class label: . print(&#39;% benign: {:.4f}&#39;.format(sum(train_df[&#39;target&#39;] == 0) / len(train_df))) print(&#39;% malign: {:.4f}&#39;.format(sum(train_df[&#39;target&#39;] == 1) / len(train_df))) . % benign: 0.9824 % malign: 0.0176 . Upon analyzing the dataset, it is observed that . Target class distribution is not balanced, and more samples belong to the benign (majority) class than the malign (minority) class . | If we directly split the dataset into a proportion of say 80:20, then it is possible that the split may not be representative of the actual dataset having the same ratio of the class labels . | This will induce a bias towards predicting the benign class label and thus significantly impact the performance of the classifier . | . In order to avoid the bias due to an imbalanced dataset and ensure the same distribution of the class labels, we employ the stratified k-fold cross-validation to obtain the same distribution of the class labels in each fold. This cross-validation ensures that we are able to make predictions on all of the data using k different models. . n_splits = 5 train_df[&#39;kfold&#39;] = -1 train_df = train_df.sample(frac=1).reset_index(drop=True) train_df_labels = train_df.target.values skf = StratifiedKFold(n_splits=n_splits) for fold_idx, (train_idx, valid_idx) in enumerate(skf.split(X=train_df, y=train_df_labels)): train_df.loc[valid_idx, &#39;kfold&#39;] = fold_idx train_df.to_csv(&#39;train_folds.csv&#39;, index=False) . Now let&#39;s create a custom data loader to load the data from the specified image paths; it is also capable of performing transformations(if required), directly at the loading stage, so we don&#39;t need to worry about the transformations at later stages. . class MelanomaDataset(torch.utils.data.Dataset): def __init__(self, image_paths, targets, resize, augmentations=None): &quot;&quot;&quot; Initialize the Melanoma Dataset Class &quot;&quot;&quot; self.image_paths = image_paths self.targets = targets self.resize = resize self.augmentations = augmentations def __getitem__(self, index): &quot;&quot;&quot; Returns the data instance from specified index location &quot;&quot;&quot; image_path = self.image_paths[index] target = self.targets[index] # open the image using PIL image = Image.open(image_path) if self.resize is not None: image = image.resize( (self.resize[1], self.resize[0]), resample=Image.BILINEAR ) image = np.array(image) # perform the augmentations if any if self.augmentations is not None: augmented = self.augmentations(image=image) image = augmented[&#39;image&#39;] # make the channel first image = np.transpose(image, (2, 0, 1)).astype(np.float32) return { &#39;image&#39;: torch.tensor(image), &#39;target&#39;: torch.tensor(target) } def __len__(self): &quot;&quot;&quot; Returns the number of examples / instances &quot;&quot;&quot; return len(self.image_paths) . Evaluation Metrics . The area under the ROC curve (AUC) was used as an evaluation metric for the problem due to an imbalanced dataset. A ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classifier at various classification thresholds. It is a measure of how well the model is capable of distinguishing between the different classes. This curve plots two parameters: . True Positive Rate (TPR) is a synonym for recall and is therefore defined as follows: | . $${TPR = frac{TP}{TP + FN}}$$ . False Positive Rate (FPR) is defined as follows: | . $${FPR = frac{FP}{FP + TN}}$$ . AUC is a measure of the area underneath the entire ROC curve. It represents the degree of separability. It ranges in value from 0 to 1. The higher the AUC, the better the model is at distinguishing classes. . For more details, please refer to the Classification: ROC Curve and AUC by Google’s Machine Learning Crash Course. . Losses . We use the Binary Cross Entropy (BCE) loss for the problem since here we need to classify the images into classes: benign or malignant. The formula of the BCE loss is as given below: . $$ L = - frac{1}{N} sum_{i=1}^N{(y_i log(p_i) + (1 - y_i) log(1 - p_i))} $$where $y_i$ is the class label (0 for benign and 1 for malign) and $p_i$ is the predicted probability of the image being malign for the $i^{th}$ sample . We will use the nn.BCEWithLogitsLoss directly from the PyTorch&#39;s nn module. . . Another loss that we try for the problem is the Focal loss, an extension of BCE loss that tries to handle class imbalance by penalizing the misclassified examples. It is expressed as follows: . $$ L = - alpha_t(1 - p_t)^ gamma log(p_t $$$$ alpha_t= left { begin{matrix} alpha &amp; if ; y = 1 1 - alpha &amp; otherwise end{matrix} right. $$where $ gamma$ is a prefixed positive scalar value and $ alpha$ is a prefixed value between 0 and 1 to balance the positive labeled samples and negative labeled samples . class FocalLoss(nn.Module): def __init__(self, alpha=1, gamma=2): &quot;&quot;&quot; Initialize the Focal Loss Class &quot;&quot;&quot; super(FocalLoss, self).__init__() self.alpha = alpha self.gamma = gamma def forward(self, predictions, targets): &quot;&quot;&quot; Calculates the Focal Loss &quot;&quot;&quot; criterion = nn.BCEWithLogitsLoss() logits = criterion(predictions, targets.view(-1, 1).type_as(predictions)) pt = torch.exp(-logits) focal_loss = self.alpha * (1 - pt) ** self.gamma * logits return torch.mean(focal_loss) . Network . Convolutional Neural Networks are very good at the task of image processing and classifications due to the following reasons: . Require fewer parameters i.e. less complex than feed forward networks (FFNs) but are able to achieve as efficient or even better performance | Able to identify the low-level features such as edges as well as high-level features such as objects or patterns | . Here we try the following two different network architectures: . EfficientNet | Squeeze and Excitation Network | . EfficientNet . The EfficientNet architecture by Tan et al. focuses on scaling up the performance of traditional CNNs in terms of accuracy and at the same time, focuses on building a more computationally efficient architecture. . How can CNNs be Scaled up? . . Types of Model Scaling (image source: https://arxiv.org/pdf/1905.11946.pdf) Here compound scaling is the method proposed by Tan et al. . Let’s first analyze how traditional scaling works and why each type of scaling is necessary. . Width Scaling (w): The objective of using a wider network is that wider networks are more suited to capture more fine-grained features. This is typically used in shallow networks. But the problem is that if we make the network extremely wide, the performance of the network in terms of accuracy degrades. Therefore, we need an optimum width to maintain performance. . | Depth Scaling (d): Theoretically, deeper neural networks tend to capture more complex features and this makes the neural network generalize well to other tasks. But practically, if we go on making the network too deep, it will increase the computational complexity and such networks will require huge training times. Also very deep neural networks suffer from vanishing/exploding gradient problems. Therefore, we need an optimum depth to achieve good performance. . | Resolution Scaling (r): By intuition, we can consider that if we take a high-resolution image, it would yield more fine-grained features and thus would boost the performance. Though this is true to a certain extent, we cannot assume a linear relationship between these. This is because the accuracy gain diminishes very quickly. So to a certain extent, by resolution scaling, we can improve the performance of the network. . | . Based on their study, the authors have considered that all these 3 factors should be considered to a certain extent and a combined scaling technique must be incorporated. . By intuition, if we are considering a high-resolution image, naturally, we have to increase the depth and the width of the network. To validate this intuition, the authors considered a fixed-width network (w) and varied the scaling factors r and d. It was observed that the accuracy improved when high-resolution images were passed through deeper neural networks. . The authors have proposed a scaling technique which uses a compound coefficient $ phi$ in order to scale the width, depth and resolution of the network in a uniform fashion, which is expressed as follows: . $${depth: d = alpha^ phi}$$ $${width: w = beta^ phi}$$ $${resolution: r = gamma^ phi}$$ . $$such that alpha cdot beta^2 cdot gamma^2 approx2 and alpha, beta, gamma geq 1$$ . where $ phi$ is a use r-specified coefficient which can control how many resources are available and $ alpha$, $ beta$, $ gamma$ controls depth, width, image resolution, respectively. . Firstly, for B0, the authors have fixed $ phi = 1$ and have assumed that twice more resources are available and have performed a small grid search for the other parameters. The optimal values which satisfy $ alpha cdot beta^2 cdot gamma^2 approx2$, were found out to be $ alpha = 1.2$, $ beta = 1.1$ and $ gamma = 1.15$. . Later, the authors kept these values of $ alpha$, $ beta$, $ gamma$ as constant and experimented with different values of $ phi$. The authors experiment with different values of $ phi$ to produce the variants EfficientNets B1-B7. . For more details, please refer to the EfficientNet paper. . class Net(nn.Module): def __init__(self, variant=&#39;efficientnet-b2&#39;): &quot;&quot;&quot; Initializes pretrained EfficientNet model &quot;&quot;&quot; super(Net, self).__init__() self.base_model = EfficientNet.from_pretrained(variant) self.fc = nn.Linear(self.base_model._fc.in_features, 1) def forward(self, image, target): &quot;&quot;&quot; Returns the result of forward propagation &quot;&quot;&quot; batch_size, _, _, _ = image.shape out = self.base_model.extract_features(image) out = F.adaptive_avg_pool2d(out, 1).view(batch_size, -1) out = self.fc(out) # loss = nn.BCEWithLogitsLoss()(out, target.view(-1, 1).type_as(out)) loss = FocalLoss()(out, target.view(-1, 1).type_as(out)) return out, loss model = Net() . Squeeze and Excitation Networks . Traditional convolutional neural networks (CNNs) use convolution operation which fuses information both spatially and channel-wise, but Jie Hu et al. proposed a novel architecture Squeeze and Excitation Networks (SENets) in the 2017 ImageNet challenge that focuses on the channel-wise information correlation. This network improved the results from the previous year by 25%. . The basic intuition behind this approach was to adjust the feature map channel-wise by adding the parameters to each channel of a convolutional block. These parameters represent the relevance of each feature map to the information, much like we use attention in the recurrent neural networks (RNNs). . . Squeeze and Excitation Block (image source: https://arxiv.org/pdf/1709.01507.pdf) The above figure represents the Squeeze-and-Excitation (SE) block where it performs a series of operations: squeeze and excitation, which allows the network to recalibrate the channel-wise information i.e. emphasize informative feature maps and suppresses less useful feature maps. The squeeze operation produces a channel descriptor expressive of the whole image by aggregating feature maps across the spatial dimensions using global average pooling. The excitation operation produces channel-wise relevance using the two fully-connected (FC) layers where the FC captures channel-wise dependencies. This block can be directly applied to the existing architectures such as ResNet, which is shown below. . . Residual module (left) and SE ResNet module (right) (image source: https://arxiv.org/pdf/1709.01507.pdf) The computational overhead of the network depends on where you apply the SE block. There was a minor increase in the computational overhead, which is feasible compared to the performance boost achieved from the network. The authors applied the SE block at earlier layers to reduce the computation overhead since, at later layers, the number of parameters increases as the feature maps increase channel-wise. . For more details, please refer to the Squeeze-and-Excitation Networks paper. . class Net(nn.Module): def __init__(self): &quot;&quot;&quot; Initializes pretrained EfficientNet model &quot;&quot;&quot; super(Net, self).__init__() self.base_model = pretrainedmodels.se_resnext50_32x4d(pretrained=&#39;imagenet&#39;) self.fc = nn.Linear(2048, 1) def forward(self, image, target): &quot;&quot;&quot; Returns the result of forward propagation &quot;&quot;&quot; batch_size, _, _, _ = image.shape out = self.base_model.features(image) out = F.adaptive_avg_pool2d(out, 1).view(batch_size, -1) out = self.fc(out) # loss = nn.BCEWithLogitsLoss()(out, target.view(-1, 1).type_as(out)) loss = FocalLoss()(out, target.view(-1, 1).type_as(out)) return out, loss model = Net() . Training &amp; Prediction . Here we use early stopping and learning rate scheduler for training the model faster. . def train(fold): &quot;&quot;&quot; Train the model on a fold &quot;&quot;&quot; n_epochs = 50 train_bs = 32 valid_bs = 16 best_score = -np.Inf es_patience = 5 patience = 0 model_path = &#39;./model_fold_{:02d}.pth&#39;.format(fold) train_folds_df = pd.read_csv(train_folds_df_path) train_df = train_folds_df[train_folds_df.kfold != fold].reset_index(drop=True) valid_df = train_folds_df[train_folds_df.kfold == fold].reset_index(drop=True) train_images = train_df.image_path.values train_targets = train_df.target.values valid_images = valid_df.image_path.values valid_targets = valid_df.target.values model = Net() model.to(device) mean = (0.485, 0.456, 0.406) std = (0.229, 0.224, 0.225) # augmentations for train and validation images train_aug = albumentations.Compose([ albumentations.Normalize(mean, std, max_pixel_value=255.0, always_apply=True), albumentations.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15), albumentations.Flip(p=0.5), ]) valid_aug = albumentations.Compose([ albumentations.Normalize(mean, std, max_pixel_value=255.0, always_apply=True), ]) # creating dataset and dataloader for train and validation images train_dataset = MelanomaDataset( image_paths=train_images, targets=train_targets, resize=None, augmentations=train_aug ) train_loader = torch.utils.data.DataLoader( train_dataset, batch_size=train_bs, shuffle=True, num_workers=4 ) valid_dataset = MelanomaDataset( image_paths=valid_images, targets=valid_targets, resize=None, augmentations=valid_aug ) valid_loader = torch.utils.data.DataLoader( valid_dataset, batch_size=valid_bs, shuffle=False, num_workers=4 ) optimizer = torch.optim.Adam(model.parameters(), lr=3e-4) scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( optimizer, patience=3, threshold=0.001, mode=&#39;max&#39; ) for epoch in range(n_epochs): train_loss = 0 valid_loss = 0 train_steps = 0 valid_steps = 0 # model in train mode model.train() tk0 = tqdm(train_loader, total=len(train_loader), position=0, leave=True) with torch.set_grad_enabled(True): for idx, data in enumerate(tk0): # load tensor to GPU for key, value in data.items(): data[key] = value.to(device) # forward pass _, loss = model(**data) # backward pass, optimize optimizer.zero_grad() loss.backward() optimizer.step() train_loss += loss.item() train_steps += 1 # update progress bar tk0.set_postfix(loss=train_loss/train_steps) tk0.close() # model in eval mode model.eval() val_predictions = np.zeros((len(valid_df), 1), dtype=np.float32) tk0 = tqdm(valid_loader, total=len(valid_loader), position=0, leave=True) with torch.no_grad(): for idx, data in enumerate(tk0): # load tensor to GPU for key, value in data.items(): data[key] = value.to(device) # model prediction batch_preds, loss = model(**data) start = idx * valid_bs end = start + len(data[&#39;image&#39;]) val_predictions[start:end] = batch_preds.cpu() valid_loss += loss.item() valid_steps += 1 # update progress bar tk0.set_postfix(loss=valid_loss/valid_steps) tk0.close() # schedule learning rate auc = roc_auc_score(valid_df.target.values, val_predictions.ravel()) print(&#39;Epoch = {} , AUC = {}&#39;.format(epoch, auc)) scheduler.step(auc) # early stopping if best_score &lt; auc: print(&#39;Validation score improved ({} -&gt; {}). Saving Model!&#39;.format(best_score, auc)) best_score = auc patience = 0 torch.save(model.state_dict(), model_path) else: patience += 1 print(&#39;Early stopping counter: {} out of {}&#39;.format(patience, es_patience)) if patience == es_patience: print(&#39;Early stopping! Best AUC: {}&#39;.format(best_score)) break . def predict(fold): &quot;&quot;&quot; Model predictions on a fold &quot;&quot;&quot; test_bs = 16 model_path = &#39;./model_fold_{:02d}.pth&#39;.format(fold) test_df = pd.read_csv(test_df_path) test_images = test_df.image_path.values test_targets = np.zeros(len(test_images)) model = Net() model.load_state_dict(torch.load(model_path)) model.to(device) mean = (0.485, 0.456, 0.406) std = (0.229, 0.224, 0.225) # test augmentation on test images test_aug = albumentations.Compose([ albumentations.Normalize(mean, std, max_pixel_value=255.0, always_apply=True), ]) # dataset and dataloader for test images test_dataset = MelanomaDataset( image_paths=test_images, targets=test_targets, resize=None, augmentations=test_aug ) test_loader = torch.utils.data.DataLoader( test_dataset, batch_size=test_bs, shuffle=False, num_workers=4 ) # model in eval mode model.eval() test_predictions = np.zeros((len(test_df), 1)) tk0 = tqdm(test_loader, total=len(test_loader), position=0, leave=True) with torch.no_grad(): for idx, data in enumerate(tk0): # load tensor to GPU for key, value in data.items(): data[key] = value.to(device) batch_preds, _ = model(**data) start = idx * test_bs end = start + len(data[&#39;image&#39;]) test_predictions[start:end] = batch_preds.cpu() tk0.close() return test_predictions.ravel() . Now let’s train each fold and save the best model: . for i in range(n_splits): train(i) . Great, now we are ready with our models so let’s predict the targets on the test images: . final_predictions = np.zeros((len(test_df), 1)).ravel() for i in range(n_splits): final_predictions += predict(i) final_predictions /= n_splits . sample = pd.read_csv(&#39;./sample_submission.csv&#39;) sample.loc[:, &#39;target&#39;] = final_predictions sample.to_csv(&#39;submission.csv&#39;, index=False) . Results . Here, we had trained 2 models, SEResNeXt50_32x4d and the B2 variant of the EfficientNet model. Both models were trained using the loss functions BCE Loss and Focal Loss and the results are compared and tabulated as follows: . Model BCE Loss Focal Loss . SEResNeXt50_32x4d | 0.8934 | 0.8762 | . EfficientNet B2 | 0.8972 | 0.8921 | . SEResNeXt50_32x4d + EfficientNet B2 | 0.9019 | - | . In the 3rd case, we average out the predictions of both the models and assess the performance. . Future Resources . Kaggle notebooks are a great place to learn and adapt to best practices of the experts. Here are the few kernels from the competition you can refer: . SIIM: d3 EDA, Augmentations and ResNeXt | Analysis of Melanoma Metadata and EffNet Ensemble | Triple Stratified KFold with TFRecords | . References . Melanoma Overview | SIIM-ISIC Melanoma Classification | Preprocessed SIIC ISIC Images by Arnaud Roussel | Classification: ROC Curve and AUC | Focal Loss Paper | EfficientNet Paper | EfficientNet Blog by Aakash Nain | Squeeze and Excitation Networks Paper | .",
            "url": "https://jaygala24.github.io/blog/python/pytorch/kaggle-competition/2020/12/02/kaggle-melanoma-challenge.html",
            "relUrl": "/python/pytorch/kaggle-competition/2020/12/02/kaggle-melanoma-challenge.html",
            "date": " • Dec 2, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Neural Style Transfer",
            "content": "This post assumes that you have basic skills of working with PyTorch. If you are new to PyTorch, I would highly encourage you to go through Deep Leaning With PyTorch: A 60 Minute Blitz by PyTorch. It’s a great place for beginners to get your hands dirty. . Neural Style Transfer is an algorithm developed by Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge that blends the content of one image with the style of another image using Deep Neural Networks to create artistic images of high perceptual quality. . Intuition . Convolutional Neural Networks are very powerful, extracting the visual information hierarchically. This makes them really useful for this task. The lower layers care more about the detailed pixel values, whereas the higher layers care more about the actual content of the image (objects such as eyes, nose, etc.). . . Convolutional Neural Network (CNN) (image source: https://arxiv.org/pdf/1508.06576.pdf) In the above figure, the output image is a mix of two since we use the activations of the neural network at specific layers as a filter to get the intermediate style and content output of the inputs. . The principle underlying the neural style transfer is simple: . Define two distances, one for the content and one for style. | Measure how different the content and style are between two images, respectively. | Reconstruct the image from white noise using backpropagation by minimizing both content and style distance with the content and style images, respectively. | . Losses Involved . Content loss: $$L_{content}( bar{p}, bar{x}, bar{l}) = frac{1}{2} sum_{i,j}(F_{ij}^l - P_{ij}^l)^2$$ where, . $ bar{p}$ and $ bar{x}$ are the content and generated images respectively. | $F_{i,j}$ and $P_{i,j}$ are the feature representation of the original and generated image of $i^{th}$ filter at position $j$ in layer $l$ respectively. | . | Style loss: $$E_{l} = frac{1}{4N_{l}^2M_{l}^2} sum_{i,j}(G_{ij}^l - A_{ij}^l)^2$$ $$L_{style}( bar{a}, bar{x}) = sum_{i=0}^{L}w_{l}E_{l}$$ where, . $ bar{a}$ and $ bar{x}$ are the style and generated image respectively. | $A^{l}$ and $G^{l}$ are the style representation (Gram Matrix) at layer $l$ respectively. | $w_{l}$ and $E_{l}$ are weighing factor and error for specific layer ${l}$ respectively. | . | Total loss, which is a weighted sum of the two above: $$L_{total}( bar{p}, bar{a}, bar{x}) = alpha L_{content}( bar{p}, bar{x}) + beta L_{style}( bar{a}, bar{x})$$ where, . $L_{content}$ and $L_{style}$ are the content and style loss respectively. | $ alpha$ and $ beta$ are weights for the content and style loss, respectively. | $ bar{p}$, $ bar{a}$ $ bar{x}$ are the content, style and generated images respectively. | . | . There is a trade-off between the actual content and artistic style, which is determined by $ alpha$ and $ beta$. If the content is more important, then increase the $ alpha$. If the style is more important, then increase the $ beta$. . Code Walkthrough . Now let’s go to the implementation of the above algorithm by importing the below packages: . %matplotlib inline import torch import torch.nn as nn import torch.nn.functional as F from torchvision import transforms, models from PIL import Image import numpy as np import matplotlib.pyplot as plt . Now, we select the device on which our network will run. Neural style transfer algorithm runs faster on GPU so check if GPU is available using torch.cuda.is_available(). . device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) . Now, let’s download and load the pre-trained VGG19 model. VGG is trained for the task of object detection. We freeze all VGG parameters as we are using it for optimizing the target image. . vgg = models.vgg19(pretrained=True).features # move the vgg model to GPU in eval mode (freeze model parameters) if available vgg.to(device).eval() . Sequential( (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU(inplace=True) (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (3): ReLU(inplace=True) (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (6): ReLU(inplace=True) (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (8): ReLU(inplace=True) (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (11): ReLU(inplace=True) (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (13): ReLU(inplace=True) (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (15): ReLU(inplace=True) (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (17): ReLU(inplace=True) (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (20): ReLU(inplace=True) (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (22): ReLU(inplace=True) (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (24): ReLU(inplace=True) (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (26): ReLU(inplace=True) (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (29): ReLU(inplace=True) (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (31): ReLU(inplace=True) (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (33): ReLU(inplace=True) (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (35): ReLU(inplace=True) (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) ) . Now, let’s load the style and content images. The PIL images loaded have values between 0 to 255, but when they are transformed into torch tensors, their values are converted between 0 and 1. We perform few transformations such as Resize(), ToTensor(), Normalize() on the image. . imsize = 512 if torch.cuda.is_available() else 128 # use small size if no gpu # VGG19 mean and std for each channel cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]) cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]) loader = transforms.Compose([ # scale imported image transforms.Resize(imsize), # transform it into a torch tensor transforms.ToTensor(), # normalize the tensor as per VGG network transforms.Normalize(mean=cnn_normalization_mean, std=cnn_normalization_std) ]) def load_image(image_path, transform=None): &quot;&quot;&quot; Load an image and comvert it to a torch tensor. &quot;&quot;&quot; image = Image.open(image_path) if transform: # transform the image image = transform(image) # add a fake batch dimension to fit network&#39;s input dimension image = image.unsqueeze(0) return image . style_img = load_image(&#39;./images/style.jpg&#39;, transform=loader).to(device) content_img = load_image(&#39;./images/content.jpg&#39;, transform=loader).to(device) . Now, let’s create a function to denormalize the image tensors, which will be later helpful to display the image tensors. . def denorm_image(img_tensor): &quot;&quot;&quot; Denormalize the image for visualization &quot;&quot;&quot; # clone the image tensor and detach from tracking image = img_tensor.to(&#39;cpu&#39;).clone().detach() # remove the fake batch dimension image = image.numpy().squeeze() # reshape (n_C, n_H, n_W) -&gt; (n_H, n_W, n_C) image = image.transpose(1, 2, 0) # denormalize the image image = image * cnn_normalization_std.numpy() + cnn_normalization_mean.numpy() # restrict the value between 0 and 1 by clipping the outliers image = image.clip(0, 1) return image . Now, let’s display the content and style image. . fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(16, 8)) ax1.imshow(denorm_image(content_img)) ax1.set_title(&#39;Content&#39;, fontsize=20) ax2.imshow(denorm_image(style_img)) ax2.set_title(&#39;Style&#39;, fontsize=20) plt.show() . Now, let’s select the convolutional layers from VGG19 to extract the feature maps. . def get_feature_maps(image, model, layers=None): &quot;&quot;&quot; Extract the convolutional feature maps from conv1_1 ~ conv5_1 &quot;&quot;&quot; if layers is None: # layer number for conv1_1 ~ conv5_1 layers = [&#39;0&#39;, &#39;5&#39;, &#39;10&#39;, &#39;19&#39;, &#39;28&#39;] # conv feature map features = [] x = image # iterate through the model layers for name, layer in model._modules.items(): x = layer(x) # checks for the layer match if name in layers: features.append(x) return features . Now, let’s define the gram matrix used in style loss, which we try to minimize during the backpropagation. . def gram_matrix(input): &quot;&quot;&quot; Calculates the gram matrix for input &quot;&quot;&quot; # a = 1 (batch_size), b = n_C (number of feature maps) # (c, d) = dimension of feature map a, b, c, d = input.size() # reshape the convolutional feature maps features = input.view(a * b, c * d) # compute the gram matrix G = torch.mm(features, features.t()) # Normalize the values of gram matrix G = G.div(a * b * c * d) return G . Now, let’s create a clone of the content image as a starting image for the target, which we transform such that the content image has an artistic style. . target_img = content_img.clone().to(device) # Alternative way: you can start with white noise to get an image with # content attributes of content image and style attributes of style image # target_img = torch.randn(content_img.data.size()).to(device) . Now let’s run the model and try to minimize the loss using backpropagation. . def run_neural_style_transfer(model, content_img, style_img, target_img, num_steps=2000, sample_steps=400, learning_rate=0.02, style_weight=1e4, content_weight=1e-2): &quot;&quot;&quot; Run the neural style transfer &quot;&quot;&quot; # optimizer for reconstruction of content image with artistic style optimizer = torch.optim.Adam([target_img.requires_grad_()], lr=learning_rate, betas=[0.99, 0.999]) for step in range(num_steps): # extract the conv feature maps for target, content and style images target_features = get_feature_maps(target_img, model) content_features = get_feature_maps(content_img, model) style_features = get_feature_maps(style_img, model) # initialize the style and content loss style_loss = 0 content_loss = 0 # calculate the style and content loss for each specific layer for f1, f2, f3 in zip(target_features, content_features, style_features): # compute content loss with target and content images content_loss += torch.mean((f1 - f2) ** 2) # compute the gram matrix for target and style feature maps f1 = gram_matrix(f1) f3 = gram_matrix(f3) # compute the style loss with target and style images style_loss += torch.mean((f1 - f3) ** 2) # compute total loss, backprop and optimize loss = content_loss * content_weight + style_loss * style_weight optimizer.zero_grad() loss.backward() optimizer.step() if (step+1) % sample_steps == 0: # print the model stats print(&quot;run {}:&quot;.format(step+1)) print(&#39;Style Loss : {:4f} Content Loss: {:4f}&#39;.format( style_loss.item(), content_loss.item())) print() return target_img . output_img = run_neural_style_transfer(vgg, content_img, style_img, target_img) # display the style transfered image output_img = denorm_image(output_img) plt.figure() plt.imshow(output_img) plt.title(&#39;Output Image&#39;) plt.show() . run 400: Style Loss : 0.000006 Content Loss: 21.730204 run 800: Style Loss : 0.000004 Content Loss: 19.803923 run 1200: Style Loss : 0.000004 Content Loss: 19.264122 run 1600: Style Loss : 0.000004 Content Loss: 19.008064 run 2000: Style Loss : 0.000004 Content Loss: 18.843626 . Great! Now you have become an artist who can generate artworks from a content image and style image. . Future Resources . Try the Fast Neural Style Transfer, which uses Perceptual Losses for Real-Time Style Transfer and Super-Resolution along with Instance Normalization. | Try the Tensorflow Implementation of Neural Style Transfer. | . References . Neural Style Transfer Paper | Neural Style Transfer Using PyTorch | .",
            "url": "https://jaygala24.github.io/blog/python/pytorch/neural-style/2020/07/31/neural-style-transfer.html",
            "relUrl": "/python/pytorch/neural-style/2020/07/31/neural-style-transfer.html",
            "date": " • Jul 31, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Guide to NumPy For Scientific Computing",
            "content": "This post assumes that you have the necessary skills to work with Python. If you are new to Python, I would highly encourage you to go through Google’s Python Class first. It’s an excellent place for beginners and also offers exercises to get your hands dirty. . Python is a great general-purpose programming language, but it becomes really convenient and powerful for Machine Learning and Data Science with a few popular libraries. The libraries provide efficient code optimization and memory management, along with some additional features and functionalities. . Introduction . Although all the computations can be done by Python on its own stand-alone, these libraries provide a much efficient way of doing the computations. . Let’s go through an example to understand the importance of these libraries. . Create a list of numbers from 1 to 5 and multiply the elements by 2: . l = [1, 2, 3, 4, 5] print(l) res = l * 2 print(res) # Notice something different . [1, 2, 3, 4, 5] [1, 2, 3, 4, 5, 1, 2, 3, 4, 5] . Multiplying the list by x concatenates the elements of the list x times. We’ll have to write a function for multiplying the elements of the list by 2. . def multiply_2(arr): &quot;&quot;&quot; Multiply elements of list by 2 Parameters: arr - list Input list &quot;&quot;&quot; for i in range(len(arr)): arr[i] = arr[i] * 2 return arr res = multiply_2(l) print(res) . [2, 4, 6, 8, 10] . What’s the problem with the above computation? . This seems fine if we are working on data in smaller quantities, but we work with data in larger quantities, this approach becomes really inefficient. This is where these powerful libraries come in to aid us. . Don’t worry if you don’t get the syntax initially; you’ll get it as we progress through the post. . import numpy as np . a = np.array([1, 2, 3, 4, 5]) print(a) res = a * 2 print(res) . [1 2 3 4 5] [ 2 4 6 8 10] . This library uses a vectorized approach, which simply means applying the operations on whole arrays instead of individual elements. This vectorized code is highly optimized and written in C. . %%timeit multiply_2(l) . The slowest run took 4.88 times longer than the fastest. This could mean that an intermediate result is being cached. 77.5 µs ± 33 µs per loop (mean ± std. dev. of 7 runs, 100000 loops each) . %%timeit a * 2 . 995 ns ± 292 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each) . The time of execution when using these libraries is really short compared to regular python. There are many more reasons that drive us towards the usage of these libraries. But let’s get started with the usage of these libraries: . NumPy . Numpy is the core library for doing scientific computing in Python involves working the multidimensional arrays. Many libraries are built on top of NumPy. . Basics . A NumPy array is a grid of values, all of the same type. Nested python lists can be used to initialize the array. You can access elements with square brackets for the 1D array, but for the 2D array, it’s a little different. . import numpy as np . a = np.array([1, 2, 3, 4]) print(a) # Create a 2D array b = np.array([[1, 2], [3, 4]]) print(b) . [1 2 3 4] [[1 2] [3 4]] . Let’s write a function which gives us more details of these NumPy arrays: . def print_info(arr): &quot;&quot;&quot; Prints details of the numpy array Parameters: arr - nd array Input array &quot;&quot;&quot; print(&#39;number of elements:&#39;, arr.size) print(&#39;number of dimensions:&#39;, arr.ndim) print(&#39;shape:&#39;, arr.shape) print(&#39;data type:&#39;, arr.dtype) print(&#39;strides:&#39;, arr.strides) print(&#39;flags:&#39;, arr.flags) . ndarray.size: Tells us about the number of elements in a NumPy array. | ndarray.ndim: Tells us about the number of dimensions in a NumPy array. | ndarray.shape: Tells us about the size of the NumPy array along each dimension. | ndarray.dtype: Tells us about the data type of elements in a NumPy array. | ndarray.strides: Tells us about the no. of bytes need to step in each dimension to access the adjacent element. Strides will be multiples of 8 along each dimension. | ndarray.flags: Tells us about how the NumPy array is stored in memory. C_CONTIGUOUS tells us that elements in the memory are row-wise. F_CONTIGUOUS tells us that elements in the memory are column-wise. | . print_info(a) . number of elements: 4 number of dimensions: 1 shape: (4,) data type: int64 strides: (8,) flags: C_CONTIGUOUS : True F_CONTIGUOUS : True OWNDATA : True WRITEABLE : True ALIGNED : True WRITEBACKIFCOPY : False UPDATEIFCOPY : False . print_info(b) . number of elements: 4 number of dimensions: 2 shape: (2, 2) data type: int64 strides: (16, 8) flags: C_CONTIGUOUS : True F_CONTIGUOUS : False OWNDATA : True WRITEABLE : True ALIGNED : True WRITEBACKIFCOPY : False UPDATEIFCOPY : False . Numpy arrays are referenced based. They point to the same array in memory when you assign a defined array to another variable. So you should be careful that you don’t modify the information in place, which may be useful for later purposes. . a = np.array([0, 1, 2, 3, 4]) print(&#39;a:&#39;, a) b = a a[0] = 5 print(&#39;b:&#39;, b) a[0] = 0 . a: [0 1 2 3 4] b: [5 1 2 3 4] . Numpy provides a copy method to create a copy of the same array in memory. . print(&#39;a:&#39;, a) b = a.copy() a[0] = 5 print(&#39;b:&#39;, b) . a: [0 1 2 3 4] b: [0 1 2 3 4] . Numpy also provides many functions to create arrays. Most of the functions take shape as a parameter. . a = np.ones((1, 3)) print(&#39;a:&#39;, a) # Create an array of all zeros b = np.zeros((1, 3)) print(&#39;b:&#39;, b) # Create a constant array c = np.full((1, 3), 4) print(&#39;c:&#39;, c) # Create an identity matrix of 3x3 d = np.eye(3) print(&#39;d:&#39;, d) # Create an array of random values e = np.random.random((1, 3)) print(&#39;e:&#39;, e) # Create an array of random values from uniform distribution f = np.random.rand(3) print(&#39;f:&#39;, f) # Create an array of random values from normal distribution g = np.random.randn(3) print(&#39;g:&#39;, g) . a: [[1. 1. 1.]] b: [[0. 0. 0.]] c: [[4 4 4]] d: [[1. 0. 0.] [0. 1. 0.] [0. 0. 1.]] e: [[0.22551855 0.18974127 0.87953329]] f: [0.54819848 0.01511915 0.5365319 ] g: [-0.87897997 -1.95487666 0.62275 ] . Numpy array provides different numeric datatypes options to construct the arrays. This can be really useful when you have a large dataset, so you can set the datatype based on the data limits to be memory efficient. . a = np.array([[1, 0], [0, 1]]) print(&#39;Datatype of a:&#39;, a.dtype) b = np.array([[1.0, 0], [0, 1.0]]) print(&#39;Datatype of b:&#39;, b.dtype) # Explicitly specify the datatype c = np.array([[1, 0], [0, 1]], dtype=np.int32) print(&#39;Datatype of c:&#39;, c.dtype) d = np.array([[1.0, 0], [0, 1.0]], dtype=np.float32) print(&#39;Datatype of d:&#39;, d.dtype) . Datatype of a: int64 Datatype of b: float64 Datatype of c: int32 Datatype of d: float32 . Slicing . Although slicing is similar to Python lists, there’s a slight difference for multi-dimensional arrays. We need to specify the slice for each dimension of the array. . Let’s walk through some examples to get a better understanding: . a = np.array([[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15]]) print(a) . [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11] [12 13 14 15]] . a[:2, :] # same as a[:2] . array([[0, 1, 2, 3], [4, 5, 6, 7]]) . a[:2, :2] . array([[0, 1], [4, 5]]) . a[::2, ::2] . array([[ 0, 2], [ 8, 10]]) . Now try slicing the four center elements of the array: . a[1:3, 1:3] . array([[ 5, 6], [ 9, 10]]) . a[[1, 1, 2, 2], [1, 2, 1, 2]] . array([ 5, 6, 9, 10]) . Now try accessing the elements 1, 2, 4, 7 of the array such that the dimension is not reduced: . a[[[0, 0], [1, 1]], [[1, 2], [0, 3]]] . array([[1, 2], [4, 7]]) . a[[[0], [1]], [[1, 2], [0, 3]]] . array([[1, 2], [4, 7]]) . Note that the alternative way uses broadcasting, which will be discussed later in the post. Things will be much clearer there, so please have a bit of patience. . Functions and Aggregations . Numpy comes with a lot of built-in functions, which are useful for performing various computations efficiently. One can perform arithmetic, matrix, trigonometric, exponent, logarithm operations, and many more. . Let&#39;s go through a few of these operations: . a = np.array([[0, 1], [2, 3]]) print(&#39;a:&#39;, a) b = np.ones((2, 2)) print(&#39;b:&#39;, b) . a: [[0 1] [2 3]] b: [[1. 1.] [1. 1.]] . Arithmetic Operations . print(&#39;a + b =&#39;, np.add(a, b)) # same as print(a + b) # Elementwise difference print(&#39;a - b =&#39;, np.subtract(a, b)) # same as print(a - b) # Elementwise product print(&#39;a * b =&#39;, np.multiply(a, b)) # same as print(a * b) # Elementwise division print(&#39;a / b =&#39;, np.divide(a, b)) # same as print(a / b) # Elementwise modulo print(&#39;a % b =&#39;, np.mod(a, b)) # same as print(a % b) . a + b = [[1. 2.] [3. 4.]] a - b = [[-1. 0.] [ 1. 2.]] a * b = [[0. 1.] [2. 3.]] a / b = [[0. 1.] [2. 3.]] a % b = [[0. 0.] [0. 0.]] . a = np.array([[0, -1, 2], [-3, 4, -5], [6, -7, 8]]) # Absolute values print(&#39;|a| =&#39;, np.abs(a)) # same as print(np.absolute(a)) # Square values print(&#39;a ^ 2 =&#39;, np.square(a)) # same as print(a ** 2) # Square root values print(&#39;a ^ 0.5 =&#39;, np.sqrt(np.abs(a))) # same as print(np.abs(a) ** 0.5) . |a| = [[0 1 2] [3 4 5] [6 7 8]] a ^ 2 = [[ 0 1 4] [ 9 16 25] [36 49 64]] a ^ 0.5 = [[0. 1. 1.41421356] [1.73205081 2. 2.23606798] [2.44948974 2.64575131 2.82842712]] . Matrix Operations . a = np.array([[0, 1], [2, 3]]) # Create a matrix of shape (2,) b = np.array([2, 4]) # Dot product of vector / vector print(&#39;b x b =&#39;, np.dot(b, b)) # same as print(b.dot(b)) # Product of matrix / vector print(&#39;a x b =&#39;, np.dot(a, b)) # same as print(a.dot(b)) # Product of matrix / matrix print(&#39;a x a =&#39;, np.dot(a, a)) # same as print(a.dot(a)) . b x b = 20 a x b = [ 4 16] a x a = [[ 2 3] [ 6 11]] . Trigonometric Operations . theta = np.linspace(0, np.pi, 3) print(&#39;theta =&#39;, theta) print(&#39;sin(theta) =&#39;, np.sin(theta)) print(&#39;cos(theta) =&#39;, np.cos(theta)) print(&#39;tan(theta) =&#39;, np.tan(theta)) . theta = [0. 1.57079633 3.14159265] sin(theta) = [0.0000000e+00 1.0000000e+00 1.2246468e-16] cos(theta) = [ 1.000000e+00 6.123234e-17 -1.000000e+00] tan(theta) = [ 0.00000000e+00 1.63312394e+16 -1.22464680e-16] . x = np.array([-1, 0, 1]) print(&quot;x =&quot;, x) print(&quot;arcsin(x) =&quot;, np.arcsin(x)) print(&quot;arccos(x) =&quot;, np.arccos(x)) print(&quot;arctan(x) =&quot;, np.arctan(x)) . x = [-1 0 1] arcsin(x) = [-1.57079633 0. 1.57079633] arccos(x) = [3.14159265 1.57079633 0. ] arctan(x) = [-0.78539816 0. 0.78539816] . Exponentiation Operations . a = np.arange(1, 5) print(&#39;a =&#39;, a) print(&#39;e ^ a =&#39;, np.exp(a)) print(&#39;e ^ a - 1 =&#39;, np.expm1(a)) print(&#39;2 ^ a =&#39;, np.exp2(a)) print(&#39;10 ^ a =&#39;, np.power(10, a)) . a = [1 2 3 4] e ^ a = [ 2.71828183 7.3890561 20.08553692 54.59815003] e ^ a - 1 = [ 1.71828183 6.3890561 19.08553692 53.59815003] 2 ^ a = [ 2. 4. 8. 16.] 10 ^ a = [ 10 100 1000 10000] . Logarithmic Operations . a = np.arange(1, 5) print(&#39;a =&#39;, a) print(&#39;ln(a) =&#39;, np.log(a)) print(&#39;ln(a + 1) =&#39;, np.log1p(a)) print(&#39;log2(a) =&#39;, np.log2(a)) print(&#39;log10(a) =&#39;, np.log10(a)) . a = [1 2 3 4] ln(a) = [0. 0.69314718 1.09861229 1.38629436] ln(a + 1) = [0.69314718 1.09861229 1.38629436 1.60943791] log2(a) = [0. 1. 1.5849625 2. ] log10(a) = [0. 0.30103 0.47712125 0.60205999] . Aggregates . a = np.arange(1, 5) print(a) . [1 2 3 4] . Calling the reduce method on arithmetic functions like add returns the sum of all elements in the array. Similarly, calling the accumulate method on arithmetic functions like add returns the array of intermediate results: . print(np.add.reduce(a)) # Product of all elements of array print(np.multiply.reduce(a)) # Intermediate result of sum print(np.add.accumulate(a)) # Intermediate result of product print(np.multiply.accumulate(a)) . 10 24 [ 1 3 6 10] [ 1 2 6 24] . a = np.random.rand(2, 2) print(a) . [[0.68347343 0.56920798] [0.20602274 0.92199748]] . print(np.sum(a)) # same as print(a.sum()) # Minimum value of array print(np.min(a)) # same as print(a.min()) # Maximum value of array print(np.max(a)) # same as print(a.max()) . 2.380701625644927 0.2060227376885182 0.9219974808291727 . We can find the sum, min, and max row-wise or col-wise by specifying the axis argument. . axis = 0 specifies we are reducing rows that means we are finding row-wise | axis = 1 specifies we are reducing cols that means we are finding col-wise | . print(np.sum(a, axis=0)) # same as print(a.sum(axis=0)) # Min value of array row wise print(np.min(a, axis=0)) # same as print(a.min(axis=0)) # Max value of array row wise print(np.max(a, axis=0)) # same as print(a.max(axis=0)) . [0.88949617 1.49120546] [0.20602274 0.56920798] [0.68347343 0.92199748] . print(np.sum(a, axis=1)) # same as print(a.sum(axis=1)) # Min value of array col wise print(np.min(a, axis=1)) # same as print(a.min(axis=1)) # Max value of array col wise print(np.max(a, axis=1)) # same as print(a.max(axis=1)) . [1.25268141 1.12802022] [0.56920798 0.20602274] [0.68347343 0.92199748] . print(np.argmin(a)) print(np.argmax(a)) . 2 3 . print(np.argmin(a, axis=0)) print(np.argmax(a, axis=0)) . [1 0] [0 1] . print(np.argmin(a, axis=1)) print(np.argmax(a, axis=1)) . [1 0] [0 1] . There are various other functions such as np.mean, np.std, np.median, np.percentile, np.any, np.all which you can refer in the documentation. . Broadcasting . Broadcasting is a way that allows us to work with NumPy arrays of different shapes when performing arithmetic operations. . Rules of Broadcasting: . Array with fewer dimensions is padded with ones on the leading side | If the shape of two arrays do not match in any dimension, the array with a shape equal to 1 is stretched to match the other shape | If in any dimension the sizes disagree and neither equal to 1, then the array is not compatible, thus leading to an error. | . a = np.arange(0, 4) print(a) . [0 1 2 3] . a + 4 . array([4, 5, 6, 7]) . In the above example, there is a duplication of the scalar value 4 into the array of shape same as an array a and performs addition, which can be demonstrated in the below figure: . . a = np.arange(0, 4) print(&#39;a:&#39;, a) # Create an array of shape (4, 1) b = np.arange(0, 4).reshape(4, 1) print(&#39;b:&#39;, b) . a: [0 1 2 3] b: [[0] [1] [2] [3]] . a + b . array([[0, 1, 2, 3], [1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6]]) . In the above example, a and b both stretches row-wise and col-wise respectively, which can be demonstrated in the below figure: . . a = np.arange(1, 7).reshape(3, 2) # Create an array of shape (2,) b = np.ones((2,)) . a + b . array([[2., 3.], [4., 5.], [6., 7.]]) . In the above example, broadcasting takes for the b, which is demonstrated in the below figure: . . a = np.arange(1, 7).reshape(3, 2) # Create an array of shape (3,) b = np.ones((3,)) . a + b . ValueError Traceback (most recent call last) &lt;ipython-input-48-bd58363a63fc&gt; in &lt;module&gt; -&gt; 1 a + b ValueError: operands could not be broadcast together with shapes (3,2) (3,) . In the above example, broadcasting does not take place as the shapes are incompatible, which can be demonstrated in the below figure: . . Fancy Indexing . Fancy indexing is simply accessing multiple elements of an array at once through the use of an array of indices. It&#39;s slicing in simple ways. . Let&#39;s walk through a few examples to understand the above: . a = np.random.randint(10, size=(3, 3)) print(a) . [[4 0 1] [5 2 8] [9 8 3]] . row = np.array([0, 1, 2]) col = np.array([1, 0, 2]) a[row, col] . array([0, 5, 3]) . a[1:, col] . array([[2, 5, 8], [8, 9, 3]]) . In the above example, broadcasting is used for the row indices. . a = np.random.randint(50, size=10) print(a) . [20 21 29 30 28 12 10 3 38 1] . mask = a &gt; 25 print(mask) . [False False True True True False False False True False] . Mask means boolean indexing where the True value indicates that the element at a particular index satisfies the conditions. This mask array helps us filtering the elements, and we can access the elements from the array using the mask as an array of indices. . print(a[mask]) . [29 30 28 38] . This masking can also be useful for modifying the values of arrays that do not satisfy the constraints. Let&#39;s see an example: . a = np.random.randint(-10, 10, size=(10,)) print(a) . [ 6 4 9 9 -7 3 -8 9 5 -3] . neg_mask = a &lt; 0 print(a[neg_mask]) . [-7 -8 -3] . a[neg_mask] = 0 print(a) . [6 4 9 9 0 3 0 9 5 0] . Strides . Computer Memory is a single tape where we need to travel sequentially in order to access the data. Strides specify the number of bytes we need to travel in the memory to access the adjacent element along each dimension. Numpy arrays are stored in a contiguous block of memory, so strides become really handy. Strides are multiples of 8 and by default row-major. . a = np.random.random((3, 2)) print(a) . [[0.07495098 0.03693228] [0.48777201 0.60112812] [0.47782039 0.18343593]] . print_info(a) . number of elements: 6 number of dimensions: 2 shape: (3, 2) data type: float64 strides: (16, 8) flags: C_CONTIGUOUS : True F_CONTIGUOUS : False OWNDATA : True WRITEABLE : True ALIGNED : True WRITEBACKIFCOPY : False UPDATEIFCOPY : False . strides: (16, 8) indicates that we need to travel 16 bytes to get the adjacent element row-wise and 8 bytes to get the adjacent element col-wise. . a_T = a.T print(a_T) . [[0.07495098 0.48777201 0.47782039] [0.03693228 0.60112812 0.18343593]] . print_info(a_T) . number of elements: 6 number of dimensions: 2 shape: (2, 3) data type: float64 strides: (8, 16) flags: C_CONTIGUOUS : False F_CONTIGUOUS : True OWNDATA : False WRITEABLE : True ALIGNED : True WRITEBACKIFCOPY : False UPDATEIFCOPY : False . If you notice the strides have been reversed from (16, 8) to (8, 16), which tells us that this is just the view of array `a’. This is the reason why NumPy is memory efficient as it points to the memory instead of creating another array as compared to python. . Note that the F_CONTIGUOUS: True which means the array a_T is column-major. . np.reshape is also based on the idea of strides, which returns the view of the same array with modified strides: . b = a.reshape(6,) print(b) . [0.07495098 0.03693228 0.48777201 0.60112812 0.47782039 0.18343593] . print_info(b) . number of elements: 6 number of dimensions: 1 shape: (6,) data type: float64 strides: (8,) flags: C_CONTIGUOUS : True F_CONTIGUOUS : True OWNDATA : False WRITEABLE : True ALIGNED : True WRITEBACKIFCOPY : False UPDATEIFCOPY : False . Broadcasting also uses the concept of strides when performing arithmetic operations. Let’s see an example: . a = np.random.random((3, 2)) print(a) # Create an array of shape (10,) b = np.random.randint(10, size=4) print(b) . [[0.76096701 0.23538774] [0.31361445 0.58805459] [0.91557476 0.24585326]] [9 0 8 2] . c = a + b . ValueError Traceback (most recent call last) &lt;ipython-input-65-1674f6151070&gt; in &lt;module&gt; -&gt; 1 c = a + b ValueError: operands could not be broadcast together with shapes (3,2) (4,) . Broadcasting, by default, gives us an error as the above operation does not satisfy broadcasting rules. Let’s apply some modifications, so it satisfies the broadcasting rules, and we get the array of shape (4, 3, 2): . c = a + b[:, np.newaxis, np.newaxis] print_info(c) . number of elements: 24 number of dimensions: 3 shape: (4, 3, 2) data type: float64 strides: (48, 16, 8) flags: C_CONTIGUOUS : True F_CONTIGUOUS : False OWNDATA : True WRITEABLE : True ALIGNED : True WRITEBACKIFCOPY : False UPDATEIFCOPY : False . np.broadcast_arrays returns the broadcasted arrays of the same shape that NumPy adds together in the above code block. . a_broadcast, b_broadcast = np.broadcast_arrays(a, b[:, np.newaxis, np.newaxis]) . print_info(a_broadcast) . number of elements: 24 number of dimensions: 3 shape: (4, 3, 2) data type: float64 strides: (0, 16, 8) flags: C_CONTIGUOUS : False F_CONTIGUOUS : False OWNDATA : False WRITEABLE : True (with WARN_ON_WRITE=True) ALIGNED : True WRITEBACKIFCOPY : False UPDATEIFCOPY : False . print(a_broadcast) . [[[0.76096701 0.23538774] [0.31361445 0.58805459] [0.91557476 0.24585326]] [[0.76096701 0.23538774] [0.31361445 0.58805459] [0.91557476 0.24585326]] [[0.76096701 0.23538774] [0.31361445 0.58805459] [0.91557476 0.24585326]] [[0.76096701 0.23538774] [0.31361445 0.58805459] [0.91557476 0.24585326]]] . strides: (0, 16, 8) indicates that the virtual view of the array a in which the view of the array appears as many times as the leading shape. In the above case, it is 4 times. . print_info(b_broadcast) . number of elements: 24 number of dimensions: 3 shape: (4, 3, 2) data type: int64 strides: (8, 0, 0) flags: C_CONTIGUOUS : False F_CONTIGUOUS : False OWNDATA : False WRITEABLE : True (with WARN_ON_WRITE=True) ALIGNED : True WRITEBACKIFCOPY : False UPDATEIFCOPY : False . print(b_broadcast) . [[[9 9] [9 9] [9 9]] [[0 0] [0 0] [0 0]] [[8 8] [8 8] [8 8]] [[2 2] [2 2] [2 2]]] . strides: (8, 0, 0) indicates that the virtual array consists of every element of array b, which appears as a 2D array of shape (3, 2) . How can we manipulate the strides and shape of an array to produce a virtual array that is much bigger but using the same memory? . def repeat(arr, n): &quot;&quot;&quot; Produce a virtual array which is n times bigger than arr without extra memory usage Parameters: arr - nd array Input array n - int Size of repeated array &quot;&quot;&quot; return np.lib.stride_tricks.as_strided(arr, shape=(n,) + arr.shape, strides=(0,) + arr.strides) . repeat(np.random.random(4), 3) . array([[0.27505351, 0.06159749, 0.03617048, 0.33832092], [0.27505351, 0.06159749, 0.03617048, 0.33832092], [0.27505351, 0.06159749, 0.03617048, 0.33832092]]) . Exercises . Create a 2D array with 1 on the border and 0 inside. . X = np.ones((5, 5)) X[1:-1, 1:-1] = 0 # Alternative way using np.zeros X = np.zeros((5, 5)) X[:, [0, -1]] = 1 X[[0, -1], 1:-1] = 1 . Normalize a 5 x 5 random matrix. . X = np.random.random((5, 5)) X_mean = np.mean(X) X_std = np.std(X) X_norm = (X - X_mean) / X_std . Given a 1D array, negate all elements which are between 3 and 8 inclusive, in place. . X = np.arange(11) mask = (2 &lt; X) &amp; (X &lt; 9) X[mask] *= -1 . How to get the alternates dates corresponding to the month of July 2016? . X = np.arange(&#39;2016-07&#39;, &#39;2016-08&#39;, 2, dtype=&#39;datetime64[D]&#39;) . Create a vector of size 10 with values ranging from 0 to 1, both excluded. . X = np.linspace(0, 1, 12)[1:-1] # Alternative way X = np.linspace(0, 1, 11, endpoint=False)[1:] . How to sort an array by the nth column? . X = np.random.randint(0, 10, (4, 3)) nth_col_sort_idx = X[:, -1].argsort() X_sort = X[nth_col_sort_idx] . Find the nearest value from a given value in an array. . X = np.random.rand(10) val = 0.75 nearest = X[np.abs(X - val).argmin()] . How to accumulate elements of a vector (X) to an array (F) based on an index list (I)? . X = np.random.randint(10, size=5) I = np.random.randint(10, size=5) F = np.bincount(I, X) . Considering a four dimensions array, how to get sum over the last two axis at once? . X = np.random.random((4, 4, 4, 4)) # Flatten the two dimensions into one new_shape = X.shape[:-2] + (-1,) Y = X.reshape(new_shape).sum(axis=-1) # Alternative way # X = np.random.random((4, 4, 4, 4)) # Tuple of axis (supported for numpy 1.7.0 onwards) # Y = X.sum(axis=(-2, -1)) . Create a function to produce a sliding window view of a 1D array. . def sliding_window(arr, size=2): &quot;&quot;&quot; Produce an array of sliding window views of arr Parameters: arr - nd array Input array size - int, optional Size of sliding window &quot;&quot;&quot; N = arr.shape[0] s = arr.strides[0] return np.lib.stride_tricks.as_strided(arr, shape=(N - size + 1, size), strides=(s, s)) . Future Resources . So far, we’ve covered many of the basics of using NumPy for performing scientific computing. But there’s still a lot of material that you can learn from. To learn more about Numpy, I would definitely recommend the following: . Pandas Data Science Handbook covers much more about Numpy. But it also has other libraries such as Pandas, Matplotlib very well explained with code walkthrough. | Advanced NumPy - SciPy 2019 covers a lot of advanced material that we have not touched on in this post. | Scipy Lecture Notes is a good resource for learning libraries related to scientific computing such as NumPy, SciPy in Python | 100 NumPy Exercises is a good place to test your knowledge. | . References . Numpy | Introduction to Numerical Computing with NumPy - SciPy 2019 | Advanced NumPy - SciPy 2019 | Python Data Science Handbook | 100 NumPy Exercises | .",
            "url": "https://jaygala24.github.io/blog/python/numpy/2020/06/20/guide-to-numpy.html",
            "relUrl": "/python/numpy/2020/06/20/guide-to-numpy.html",
            "date": " • Jun 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi! I’m Jay 👋, a CS undergrad from Mumbai University. I started my programming career with C language in 2017. I have fluency in programming languages like C, Python and JavaScript. I have been working on solving problems using ML for over a year now. . Current areas of interests: . Multimodal dialogue and question answering system | Fairness and bias in language models | Interpretability | . Independent research projects: . Worked on the task of visual dialog with the aim of reducing the bias towards dialog history using mixed attention mechanisms as a part of final year project (Accepted at ICACDS 2021). | Co-authored a paper on using object detection and image processing techniques to estimate the dimension of potholes (Published at IVCNZ 2020). | Co-authored a chapter titled “Combatting COVID-19 using Object Detection Techniques” for Elsevier’s CPS-AI &amp; COVID-19 book (Accepted). | Co-authored papers on using IoT and ML effectively to automate tasks in agriculture and waste management sectors. | . You can find more about the publications on Scholar. . I have previously worked as a ML project intern at Tata Consultancy Services where I got the opportunity to work on understanding customer behavior using NLP. I have also collaborated with Prof. Pratik Kanani on a industry project focusing on anamoly detection in heart rate (pulse) using IoT and ML. I also led a team that developed a platform for conducting C programming examination in the college for over 500 students demo. . I’m also working as a mentor at DJ Unicode an open source organization by our college department where I help juniors on software development projects and as well as learn new stuffs too 😀. . You can reach out to me using Twitter or Linkedin. .",
          "url": "https://jaygala24.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jaygala24.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}